{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 for Cardiac MR Image Segmentation (2020-2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/document/d/1llawB1bKJ_XZlNxpL1k80gF_2ozV4pCX03W2XS_xPDU/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, Utility Functions for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, show, and save images with OpenCV\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "def show_image_mask(img, mask, cmap='gray'): # visualisation\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=cmap)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            basename = os.path.basename(img_path)\n",
    "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for the model's accuracy\n",
    "\n",
    "def categorical_dice(mask1, mask2, label_class=1):\n",
    "    \"\"\"\n",
    "    Dice score of a specified class between two volumes of label masks.\n",
    "    (classes are encoded but by label class number not one-hot )\n",
    "    Note: stacks of 2D slices are considered volumes.\n",
    "\n",
    "    Args:\n",
    "        mask1: N label masks, numpy array shaped (H, W, N)\n",
    "        mask2: N label masks, numpy array shaped (H, W, N)\n",
    "        label_class: the class over which to calculate dice scores\n",
    "\n",
    "    Returns:\n",
    "        volume_dice\n",
    "    \"\"\"\n",
    "    mask1_pos = (mask1 == label_class).astype(np.float32)\n",
    "    mask2_pos = (mask2 == label_class).astype(np.float32)\n",
    "    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2, Model Definement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the segmentation model, here we adopt FCN (Fully Convolutional Network)\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "\n",
    "\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    #Define a bilinear kernel according to in channels and out channels.\n",
    "    #Returns:\n",
    "    #    return a bilinear filter tensor\n",
    "    \n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    bilinear_filter = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float32)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = bilinear_filter\n",
    "    \n",
    "    return torch.from_numpy(weight)\n",
    "\n",
    "\n",
    "pretrained_net = models.vgg16_bn(pretrained=False)\n",
    "\n",
    "pretrained_net.features[0] = nn.Conv2d(1,\n",
    "                                       4,\n",
    "                                       kernel_size=(3, 3),\n",
    "                                       stride=(1, 1),\n",
    "                                       padding=(1, 1))\n",
    "pretrained_net.features[1] = nn.BatchNorm2d(4,\n",
    "                                            eps=1e-05,\n",
    "                                            momentum=0.1,\n",
    "                                            affine=True,\n",
    "                                            track_running_stats=True)\n",
    "pretrained_net.features[2] = nn.ReLU(inplace=True)\n",
    "pretrained_net.features[3] = nn.Conv2d(4,\n",
    "                                       64,\n",
    "                                       kernel_size=(3, 3),\n",
    "                                       stride=(1, 1),\n",
    "                                       padding=(1, 1))\n",
    "pretrained_net.features[4] = nn.BatchNorm2d(64,\n",
    "                                            eps=1e-05,\n",
    "                                            momentum=0.1,\n",
    "                                            affine=True,\n",
    "                                            track_running_stats=True)\n",
    "pretrained_net.features[5] = nn.ReLU(inplace=True)\n",
    "pretrained_net.features[6] = nn.MaxPool2d(kernel_size=2,\n",
    "                                          stride=2,\n",
    "                                          padding=0,\n",
    "                                          dilation=1,\n",
    "                                          ceil_mode=False)\n",
    "\n",
    "class CNNSEG(nn.Module): # Define your model\n",
    "    def __init__(self):\n",
    "        super(CNNSEG, self).__init__()\n",
    "        # fill in the constructor for your model here\n",
    "        self.stage1 = pretrained_net.features[:7]\n",
    "        self.stage2 = pretrained_net.features[7:14]\n",
    "        self.stage3 = pretrained_net.features[14:24]\n",
    "        self.stage4 = pretrained_net.features[24:34]\n",
    "        self.stage5 = pretrained_net.features[34:]\n",
    "\n",
    "        self.scores1 = nn.Conv2d(512, 4, 1)\n",
    "        self.scores2 = nn.Conv2d(512, 4, 1)\n",
    "        self.scores3 = nn.Conv2d(128, 4, 1)\n",
    "\n",
    "        self.conv_trans1 = nn.Conv2d(512, 256, 1)\n",
    "        self.conv_trans2 = nn.Conv2d(256, 4, 1)\n",
    "\n",
    "        self.upsample_8x = nn.ConvTranspose2d(4, 4, 16, 8, 4, bias=False)\n",
    "        self.upsample_8x.weight.data = bilinear_kernel(4, 4, 16)\n",
    "\n",
    "        self.upsample_2x_1 = nn.ConvTranspose2d(512, 512, 4, 2, 1, bias=False)\n",
    "        self.upsample_2x_1.weight.data = bilinear_kernel(512, 512, 4)\n",
    "\n",
    "        self.upsample_2x_2 = nn.ConvTranspose2d(256, 256, 4, 2, 1, bias=False)\n",
    "        self.upsample_2x_2.weight.data = bilinear_kernel(256, 256, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # split the pre-trained VGG16bn into 5 stages\n",
    "        # In each stage: we contain two or three convolution layers, and end with the maxpooling layer\n",
    "        # The detailed layer information for each stages are shown in the following cells\n",
    "        s1 = self.stage1(x)\n",
    "        s1 = nn.Dropout2d(p=0.5).forward(s1)\n",
    "        s2 = self.stage2(s1)\n",
    "        s2 = nn.Dropout2d(p=0.3).forward(s2)\n",
    "        s3 = self.stage3(s2)\n",
    "        s3 = nn.Dropout2d(p=0.3).forward(s3)\n",
    "        s4 = self.stage4(s3)\n",
    "        s4 = nn.Dropout2d(p=0.3).forward(s4)\n",
    "        s5 = self.stage5(s4)\n",
    "        s5 = nn.Dropout2d(p=0.3).forward(s5)\n",
    "\n",
    "        scores1 = self.scores1(s5) # output the FCN-32s\n",
    "        \n",
    "        s5 = self.upsample_2x_1(s5)\n",
    "        add1 = s5 + s4\n",
    "        scores2 = self.scores2(add1) # Output the FCN-16s\n",
    "\n",
    "        add1 = self.conv_trans1(add1)\n",
    "        add1 = self.upsample_2x_2(add1)\n",
    "        add2 = add1 + s3\n",
    "\n",
    "        output = self.conv_trans2(add2)\n",
    "        output = self.upsample_8x(output)\n",
    "\n",
    "        score3 = output # Output the FCN-8s\n",
    "\n",
    "        return output\n",
    "\n",
    "fcn = CNNSEG() # We can now create a model using your defined segmentation model\n",
    "fcn = fcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_net.features[:7]) # Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): ReLU(inplace=True)\n",
      "  (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (12): ReLU(inplace=True)\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_net.features[7:14]) # Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (16): ReLU(inplace=True)\n",
      "  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (19): ReLU(inplace=True)\n",
      "  (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_net.features[14:24]) # Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (26): ReLU(inplace=True)\n",
      "  (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (32): ReLU(inplace=True)\n",
      "  (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_net.features[24:34]) # Stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (36): ReLU(inplace=True)\n",
      "  (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (39): ReLU(inplace=True)\n",
      "  (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (42): ReLU(inplace=True)\n",
      "  (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_net.features[34:]) # Stage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(models.vgg16_bn(pretrained=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3, Train, Validation process and Test generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "\n",
    "data_train_path = './data/train'\n",
    "data_val_path = './data/val'\n",
    "\n",
    "\"\"\"\n",
    "Change the hyper-parameter settings here\n",
    "\"\"\"\n",
    "num_workers = 0 # Threads used\n",
    "batch_size = 10\n",
    "epoches = 100 # 100 for training, and 20 for fine-tuning\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.001\n",
    "\n",
    "\n",
    "train_set = TrainDataset(data_train_path)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "conv_initial = nn.Conv2d(1, 3, 1)\n",
    "conv_initial_mask = nn.Conv2d(1, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross Entropy as the loss function, and Adam as optimizer\n",
    "\n",
    "criterion = DiceLoss().to(device)\n",
    "optimizer = optim.Adam(fcn.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the train method\n",
    "\n",
    "\"\"\"\n",
    "parameters:\n",
    "load_pretrained: whether the model is trained from scratch, or continue with the checkpoint\n",
    "pretrained: (if load_pretrained is true,) the path for checkpoint file\n",
    "\"\"\"\n",
    "def train(model, load_pretrained=False, pretrained=None):\n",
    "\n",
    "    net = model.train()\n",
    "    \n",
    "    if load_pretrained:\n",
    "        net.load_state_dict(t.load(pretrained ,map_location='cpu'))\n",
    "        \n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        print('Epoch is [{}/{}]'.format(epoch + 1, epoches))\n",
    "#         if epoch % 50 == 0 and epoch != 0:\n",
    "#             for group in optimizer.param_groups:\n",
    "#                 group['lr'] *= 0.5\n",
    "\n",
    "        # Initialize the loss and accuracy here\n",
    "        train_loss = 0\n",
    "        accuracy = 0\n",
    "        count = 0\n",
    "        \n",
    "        for iteration, sample in enumerate(training_data_loader):\n",
    "            img, mask = sample\n",
    "\n",
    "            img = Variable(img.unsqueeze(1).to(device))\n",
    "            mask = Variable(mask.to(device))\n",
    "\n",
    "            pred_mask = net(img)\n",
    "            pred_mask = F.log_softmax(pred_mask, dim=1)\n",
    "            mask = mask.to(device=device, dtype=torch.int64)\n",
    "            print(pred_mask.squeeze(1).size())\n",
    "\n",
    "            loss = criterion(pred_mask.squeeze(), mask)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            mask_image = torch.argmax(pred_mask.squeeze(), dim=1)\n",
    "            mask_acc = mask.squeeze().unsqueeze(2).numpy()\n",
    "            mask_image_acc = mask_image.unsqueeze(2).numpy()\n",
    "            \n",
    "            \n",
    "#             if mask_acc.shape == (batch_size, 96, 1, 96) and mask_image_acc.shape == (batch_size, 96, 1, 96):\n",
    "            if mask_acc.shape == (batch_size, 96, 1, 96) and mask_image_acc.shape == (batch_size, 96, 1, 96):\n",
    "                accuracy += categorical_dice(mask_acc, mask_image_acc)\n",
    "                count = count + 1\n",
    "\n",
    "            if iteration==0:\n",
    "                mask_image_1=mask_image\n",
    "\n",
    "        correct_mask = cv2.imread(os.path.join(data_train_path,'mask','cmr'+str(1)+'_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "        show_image_mask(correct_mask, mask_image_1[0,...], cmap='gray')\n",
    "        print('|batch[{}/{}]|train_batch_loss {: .8f}|'.format(epoch+1, epoches, train_loss/len(training_data_loader)))\n",
    "        print(count)\n",
    "        print('accuracy: {: .8f}'.format(accuracy/count))\n",
    "    \n",
    "#     t.save(net.state_dict(), '{}.pth'.format(epoches-1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is [1/100]\n",
      "torch.Size([10, 4, 96, 96])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (368640) must match the size of tensor b (92160) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c814b5bbdca4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#      train(fcn, True, 'checkpoint.pth')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfcn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-4511fd091cfa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, load_pretrained, pretrained)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-fb3465256a6c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, targets, smooth)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mintersection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mdice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mintersection\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msmooth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msmooth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (368640) must match the size of tensor b (92160) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#      train(fcn, True, 'checkpoint.pth')\n",
    "    train(fcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2:  Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the validation method\n",
    "\n",
    "\"\"\"\n",
    "parameters:\n",
    "pretrained: the path for checkpoint file\n",
    "\"\"\"\n",
    "\n",
    "val_set = TrainDataset(data_val_path)\n",
    "val_data_loader = DataLoader(dataset=val_set, num_workers=num_workers, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "def validate(model, pretrained=None):\n",
    "    net = model.eval()\n",
    "    net.load_state_dict(t.load('{}.pth'.format(pretrained),map_location='cpu'))\n",
    "    print('{}.pth'.format(pretrained))\n",
    "    \n",
    "    # Initialize the loss and accuracy for the model\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    batch_size=2\n",
    "    count = 0\n",
    "    index = 0\n",
    "    \n",
    "    \n",
    "    for iteration, sample in enumerate(val_data_loader):\n",
    "        img, mask = sample\n",
    "        img = img.unsqueeze(1).to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        pred_mask = net(img)\n",
    "        pred_mask = F.log_softmax(pred_mask, dim=1)\n",
    "        mask = mask.to(device=device, dtype=torch.int64)\n",
    "\n",
    "        loss = criterion(pred_mask, mask)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        mask_image = torch.argmax(pred_mask.squeeze(), dim=1)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            image = cv2.imread(os.path.join(data_val_path,'image','cmr'+str(index+101+i)+'.png'), cv2.IMREAD_UNCHANGED)\n",
    "            correct_mask = cv2.imread(os.path.join(data_val_path,'mask','cmr'+str(index+101+i)+'_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "            accuracy += categorical_dice(mask[i].numpy(), mask_image[i].detach().numpy())\n",
    "            count = count + 1\n",
    "        \n",
    "            \n",
    "        index += batch_size\n",
    "        \n",
    "#         mask_acc = mask.squeeze().unsqueeze(2).numpy()\n",
    "#         mask_image_acc = mask_image.unsqueeze(2).numpy()\n",
    "\n",
    "#         if mask_acc.shape == (batch_size, 96, 1, 96) and mask_image_acc.shape == (batch_size, 96, 1, 96):\n",
    "        \n",
    "\n",
    "        if iteration==0:\n",
    "            mask_image_1=mask_image\n",
    "    \n",
    "    print(\"Val_loss is \" + str(val_loss/len(val_data_loader)))\n",
    "    print('accuracy: {: .8f}'.format(accuracy/count))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.pth\n",
      "Val_loss is 0.13812567815184593\n",
      "accuracy:  0.67761109\n",
      "99.pth\n",
      "Val_loss is 0.1465945303440094\n",
      "accuracy:  0.68247223\n",
      "99.pth\n",
      "Val_loss is 0.13415687531232834\n",
      "accuracy:  0.65683143\n",
      "99.pth\n",
      "Val_loss is 0.13745070472359658\n",
      "accuracy:  0.67395782\n",
      "99.pth\n",
      "Val_loss is 0.13602528721094131\n",
      "accuracy:  0.67123401\n",
      "99.pth\n",
      "Val_loss is 0.14036858081817627\n",
      "accuracy:  0.68825256\n",
      "99.pth\n",
      "Val_loss is 0.1447258099913597\n",
      "accuracy:  0.68020440\n",
      "99.pth\n",
      "Val_loss is 0.13563135117292405\n",
      "accuracy:  0.68575551\n",
      "99.pth\n",
      "Val_loss is 0.13812843933701516\n",
      "accuracy:  0.68054451\n",
      "99.pth\n",
      "Val_loss is 0.13324258327484131\n",
      "accuracy:  0.67273393\n",
      "99.pth\n",
      "Val_loss is 0.14259612038731576\n",
      "accuracy:  0.67849396\n",
      "99.pth\n",
      "Val_loss is 0.1345235861837864\n",
      "accuracy:  0.69584189\n",
      "99.pth\n",
      "Val_loss is 0.1426554173231125\n",
      "accuracy:  0.67828497\n",
      "99.pth\n",
      "Val_loss is 0.14759745001792907\n",
      "accuracy:  0.67691323\n",
      "99.pth\n",
      "Val_loss is 0.1391899012029171\n",
      "accuracy:  0.68805786\n",
      "99.pth\n",
      "Val_loss is 0.13544608056545257\n",
      "accuracy:  0.69209950\n",
      "99.pth\n",
      "Val_loss is 0.13200620785355568\n",
      "accuracy:  0.68686208\n",
      "99.pth\n",
      "Val_loss is 0.14511294290423393\n",
      "accuracy:  0.68079516\n",
      "99.pth\n",
      "Val_loss is 0.14063500463962555\n",
      "accuracy:  0.68275102\n",
      "99.pth\n",
      "Val_loss is 0.13575961589813232\n",
      "accuracy:  0.66987970\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "#     validate(fcn, '199')\n",
    "    for i in range(20):\n",
    "        validate(fcn, '99')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this block you are expected to write code to load saved model and deploy it to all data in test set to \n",
    "# produce segmentation masks in png images valued 0,1,2,3, which will be used for the submission to Kaggle.\n",
    "data_path = './data/test'\n",
    "num_workers = 0\n",
    "batch_size = 2\n",
    "net = fcn.eval() \n",
    "net.load_state_dict(t.load('99.pth',map_location='cpu')) # change the pretrained model path here\n",
    "test_set = TestDataset(data_path)\n",
    "test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
    "conv_initial = nn.Conv2d(1, 3, 1)\n",
    "data_save_path = './data/test/mask/'\n",
    "index = 0\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for iteration, sample in enumerate(test_data_loader):\n",
    "    img_test = sample\n",
    "\n",
    "    img_data = Variable(img_test.unsqueeze(1).to(device))\n",
    "    out = net(img_data)\n",
    "#     out = F.log_softmax(out, dim=1)\n",
    "\n",
    "     \n",
    "    mask_image = torch.argmax(out.squeeze(), dim=1)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        image = cv2.imread(os.path.join(data_path,'image','cmr'+str(index+121+i)+'.png'), cv2.IMREAD_UNCHANGED)\n",
    "        show_image_mask(image, mask_image[i], cmap='gray')\n",
    "        cv2.imwrite(os.path.join(data_save_path, 'cmr'+str(index+121+i)+'_mask.png'), mask_image[i].detach().numpy())\n",
    "    \n",
    "    index += batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4, Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    *** Credit to https://www.kaggle.com/rakhlin/fast-run-length-encoding-python ***\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def submission_converter(mask_directory, path_to_save):\n",
    "    writer = open(os.path.join(path_to_save, \"submission.csv\"), 'w')\n",
    "    writer.write('id,encoding\\n')\n",
    "\n",
    "    files = os.listdir(mask_directory)\n",
    "\n",
    "    for file in files:\n",
    "        name = file[:-4]\n",
    "        mask = cv2.imread(os.path.join(mask_directory, file), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        mask1 = (mask == 1)\n",
    "        mask2 = (mask == 2)\n",
    "        mask3 = (mask == 3)\n",
    "\n",
    "        encoded_mask1 = rle_encoding(mask1)\n",
    "        encoded_mask1 = ' '.join(str(e) for e in encoded_mask1)\n",
    "        encoded_mask2 = rle_encoding(mask2)\n",
    "        encoded_mask2 = ' '.join(str(e) for e in encoded_mask2)\n",
    "        encoded_mask3 = rle_encoding(mask3)\n",
    "        encoded_mask3 = ' '.join(str(e) for e in encoded_mask3)\n",
    "\n",
    "        writer.write(name + '1,' + encoded_mask1 + \"\\n\")\n",
    "        writer.write(name + '2,' + encoded_mask2 + \"\\n\")\n",
    "        writer.write(name + '3,' + encoded_mask3 + \"\\n\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_converter(data_save_path, './data/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
