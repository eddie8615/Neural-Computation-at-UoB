{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 for Cardiac MR Image Segmentation (2020-2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/document/d/1llawB1bKJ_XZlNxpL1k80gF_2ozV4pCX03W2XS_xPDU/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, Utility Functions for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, show, and save images with OpenCV\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "def show_image_mask(img, mask, cmap='gray'): # visualisation\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=cmap)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            basename = os.path.basename(img_path)\n",
    "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for the model's accuracy\n",
    "\n",
    "def categorical_dice(mask1, mask2, label_class=1):\n",
    "    \"\"\"\n",
    "    Dice score of a specified class between two volumes of label masks.\n",
    "    (classes are encoded but by label class number not one-hot )\n",
    "    Note: stacks of 2D slices are considered volumes.\n",
    "\n",
    "    Args:\n",
    "        mask1: N label masks, numpy array shaped (H, W, N)\n",
    "        mask2: N label masks, numpy array shaped (H, W, N)\n",
    "        label_class: the class over which to calculate dice scores\n",
    "\n",
    "    Returns:\n",
    "        volume_dice\n",
    "    \"\"\"\n",
    "    mask1_pos = (mask1 == label_class).astype(np.float32)\n",
    "    mask2_pos = (mask2 == label_class).astype(np.float32)\n",
    "    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2, Model Definement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Define the segmentation model, here we adopt FCN (Fully Convolutional Network)\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "\n",
    "\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    #Define a bilinear kernel according to in channels and out channels.\n",
    "    #Returns:\n",
    "    #    return a bilinear filter tensor\n",
    "    \n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    bilinear_filter = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float32)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = bilinear_filter\n",
    "    \n",
    "    return torch.from_numpy(weight)\n",
    "\n",
    "# Due to vgg takes 3d images, we need to add special layers to afford to gray-scaled image.\n",
    "pretrained_net = models.vgg16_bn(pretrained=False)\n",
    "\n",
    "pretrained_net.features[0] = nn.Conv2d(1,\n",
    "                                       4,\n",
    "                                       kernel_size=(3, 3),\n",
    "                                       stride=(1, 1),\n",
    "                                       padding=(1, 1))\n",
    "pretrained_net.features[1] = nn.BatchNorm2d(4,\n",
    "                                            eps=1e-05,\n",
    "                                            momentum=0.1,\n",
    "                                            affine=True,\n",
    "                                            track_running_stats=True)\n",
    "pretrained_net.features[2] = nn.ReLU(inplace=True)\n",
    "pretrained_net.features[3] = nn.Conv2d(4,\n",
    "                                       64,\n",
    "                                       kernel_size=(3, 3),\n",
    "                                       stride=(1, 1),\n",
    "                                       padding=(1, 1))\n",
    "pretrained_net.features[4] = nn.BatchNorm2d(64,\n",
    "                                            eps=1e-05,\n",
    "                                            momentum=0.1,\n",
    "                                            affine=True,\n",
    "                                            track_running_stats=True)\n",
    "pretrained_net.features[5] = nn.ReLU(inplace=True)\n",
    "pretrained_net.features[6] = nn.MaxPool2d(kernel_size=2,\n",
    "                                          stride=2,\n",
    "                                          padding=0,\n",
    "                                          dilation=1,\n",
    "                                          ceil_mode=False)\n",
    "\n",
    "class CNNSEG(nn.Module): # Define your model\n",
    "    def __init__(self):\n",
    "        super(CNNSEG, self).__init__()\n",
    "        # fill in the constructor for your model here\n",
    "        self.stage1 = pretrained_net.features[:7]\n",
    "        self.stage2 = pretrained_net.features[7:14]\n",
    "        self.stage3 = pretrained_net.features[14:24]\n",
    "        self.stage4 = pretrained_net.features[24:34]\n",
    "        self.stage5 = pretrained_net.features[34:]\n",
    "\n",
    "        self.scores1 = nn.Conv2d(512, 4, 1)\n",
    "        self.scores2 = nn.Conv2d(512, 4, 1)\n",
    "        self.scores3 = nn.Conv2d(128, 4, 1)\n",
    "\n",
    "        self.conv_trans1 = nn.Conv2d(512, 256, 1)\n",
    "        self.conv_trans2 = nn.Conv2d(256, 4, 1)\n",
    "\n",
    "        self.upsample_8x = nn.ConvTranspose2d(4, 4, 16, 8, 4, bias=False)\n",
    "        self.upsample_8x.weight.data = bilinear_kernel(4, 4, 16)\n",
    "\n",
    "        self.upsample_2x_1 = nn.ConvTranspose2d(512, 512, 4, 2, 1, bias=False)\n",
    "        self.upsample_2x_1.weight.data = bilinear_kernel(512, 512, 4)\n",
    "\n",
    "        self.upsample_2x_2 = nn.ConvTranspose2d(256, 256, 4, 2, 1, bias=False)\n",
    "        self.upsample_2x_2.weight.data = bilinear_kernel(256, 256, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # split the pre-trained VGG16bn into 5 stages\n",
    "        # In each stage: we contain two or three convolution layers, and end with the maxpooling layer\n",
    "        # The detailed layer information for each stages are shown in the following cells\n",
    "        s1 = self.stage1(x)\n",
    "        s1 = nn.Dropout2d(p=0.1).forward(s1)\n",
    "        s2 = self.stage2(s1)\n",
    "        s2 = nn.Dropout2d(p=0.1).forward(s2)\n",
    "        s3 = self.stage3(s2)\n",
    "        s3 = nn.Dropout2d(p=0.1).forward(s3)\n",
    "        s4 = self.stage4(s3)\n",
    "        s4 = nn.Dropout2d(p=0.1).forward(s4)\n",
    "        s5 = self.stage5(s4)\n",
    "        s5 = nn.Dropout2d(p=0.1).forward(s5)\n",
    "\n",
    "        scores1 = self.scores1(s5) # output the FCN-32s\n",
    "        \n",
    "        s5 = self.upsample_2x_1(s5)\n",
    "        add1 = s5 + s4\n",
    "        scores2 = self.scores2(add1) # Output the FCN-16s\n",
    "\n",
    "        add1 = self.conv_trans1(add1)\n",
    "        add1 = self.upsample_2x_2(add1)\n",
    "        add2 = add1 + s3\n",
    "\n",
    "        output = self.conv_trans2(add2)\n",
    "        output = self.upsample_8x(output)\n",
    "\n",
    "        score3 = output # Output the FCN-8s\n",
    "\n",
    "        return output\n",
    "\n",
    "fcn = CNNSEG() # We can now create a model using your defined segmentation model\n",
    "fcn = fcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define U-Net\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_channel, out_channel)\n",
    "        self.pool = nn.MaxPool2d((2,2))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "    \n",
    "    \n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_channel+out_channel, out_channel)\n",
    "        \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         Encoder\n",
    "        self.e1 = encoder_block(1, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "        \n",
    "#         Bottleneck\n",
    "        self.b = conv_block(512, 1024)\n",
    "    \n",
    "#         Decoder\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "        \n",
    "#         Classifier\n",
    "        self.outputs = nn.Conv2d(64, 4, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "        \n",
    "        b = self.b(p4)\n",
    "        \n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "        \n",
    "        outputs = self.outputs(d4)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "unet = U_Net()\n",
    "unet = unet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3, Train, Validation process and Test generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "\n",
    "data_train_path = './data/train'\n",
    "data_val_path = './data/val'\n",
    "\n",
    "\"\"\"\n",
    "Change the hyper-parameter settings here\n",
    "\"\"\"\n",
    "num_workers = 0 # Threads used\n",
    "batch_size = 10\n",
    "epoches = 100 # 100 for training, and 20 for fine-tuning\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.001\n",
    "\n",
    "\n",
    "train_set = TrainDataset(data_train_path)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "conv_initial = nn.Conv2d(1, 3, 1)\n",
    "conv_initial_mask = nn.Conv2d(1, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross Entropy as the loss function, and Adam as optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(unet.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img shape: (10, 96, 1, 96)\n",
      "img shape: (10, 96, 1, 96)\n",
      "img shape: (10, 96, 1, 96)\n",
      "img shape: (10, 96, 1, 96)\n",
      "img shape: (10, 96, 1, 96)\n",
      "img shape: (10, 96, 1, 96)\n",
      "img shape: (10, 96, 1, 96)\n",
      "img shape: (10, 96, 1, 96)\n",
      "img shape: (10, 96, 1, 96)\n",
      "img shape: (10, 96, 1, 96)\n"
     ]
    }
   ],
   "source": [
    "# Understanding torch pipeline\n",
    "\n",
    "for i, sample in enumerate(training_data_loader):\n",
    "    img, mask = sample\n",
    "    img = Variable(img.unsqueeze(1).to(device))\n",
    "    mask = Variable(mask.to(device))\n",
    "    print('img shape:', mask.squeeze().unsqueeze(2).cpu().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the train method\n",
    "\n",
    "\"\"\"\n",
    "parameters:\n",
    "load_pretrained: whether the model is trained from scratch, or continue with the checkpoint\n",
    "pretrained: (if load_pretrained is true,) the path for checkpoint file\n",
    "\"\"\"\n",
    "def train(model, load_pretrained=False, pretrained=None):\n",
    "\n",
    "    net = model.train()\n",
    "    \n",
    "    if load_pretrained:\n",
    "        net.load_state_dict(t.load(pretrained ,map_location=device))\n",
    "        \n",
    "    print(device)\n",
    "    for epoch in range(epoches):\n",
    "        print('Epoch is [{}/{}]'.format(epoch + 1, epoches))\n",
    "#         if epoch % 50 == 0 and epoch != 0:\n",
    "#             for group in optimizer.param_groups:\n",
    "#                 group['lr'] *= 0.5\n",
    "\n",
    "        # Initialize the loss and accuracy here\n",
    "        train_loss = 0\n",
    "        accuracy = 0\n",
    "        count = 0\n",
    "        \n",
    "        for iteration, sample in enumerate(training_data_loader):\n",
    "            img, mask = sample\n",
    "\n",
    "            img = Variable(img.unsqueeze(1).to(device))\n",
    "            mask = Variable(mask.to(device))\n",
    "\n",
    "            img = img/ 255\n",
    "            pred_mask = net(img)\n",
    "            pred_mask = F.log_softmax(pred_mask, dim=1)\n",
    "            mask = mask.to(device=device, dtype=torch.int64)\n",
    "\n",
    "            loss = criterion(pred_mask, mask)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            mask_image = torch.argmax(pred_mask.squeeze(), dim=1)\n",
    "            mask_acc = mask.squeeze().unsqueeze(2).cpu().numpy()\n",
    "            mask_image_acc = mask_image.unsqueeze(2).cpu().numpy()\n",
    "#             if iteration == 0:\n",
    "#                 print('mask_img_shape', mask_image.shape)\n",
    "#                 print('mask_acc', mask_acc.shape)\n",
    "#                 print('mask_image_acc', mask_image_acc.shape)\n",
    "            \n",
    "            \n",
    "#             if mask_acc.shape == (batch_size, 96, 1, 96) and mask_image_acc.shape == (batch_size, 96, 1, 96):\n",
    "#             if mask_acc.shape == (batch_size, 96, 1, 96) and mask_image_acc.shape == (batch_size, 96, 1, 96):\n",
    "            accuracy += categorical_dice(mask_acc, mask_image_acc)\n",
    "            count = count + 1\n",
    "\n",
    "            if iteration==0:\n",
    "                mask_image_1=mask_image\n",
    "\n",
    "#         correct_mask = cv2.imread(os.path.join(data_train_path,'mask','cmr'+str(1)+'_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "#         show_image_mask(correct_mask, mask_image_1[0,...], cmap='gray')\n",
    "        print('|batch[{}/{}]|train_batch_loss {: .8f}|'.format(epoch+1, epoches, train_loss/len(training_data_loader)))\n",
    "        print('accuracy: {: .8f}'.format(accuracy/count))\n",
    "    \n",
    "    t.save(net.state_dict(), '{}.pth'.format('exp'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch is [1/100]\n",
      "|batch[1/100]|train_batch_loss  0.41716284|\n",
      "accuracy:  0.00036415\n",
      "Epoch is [2/100]\n",
      "|batch[2/100]|train_batch_loss  0.38542685|\n",
      "accuracy:  0.02122216\n",
      "Epoch is [3/100]\n",
      "|batch[3/100]|train_batch_loss  0.36792209|\n",
      "accuracy:  0.08608080\n",
      "Epoch is [4/100]\n",
      "|batch[4/100]|train_batch_loss  0.35547713|\n",
      "accuracy:  0.11252111\n",
      "Epoch is [5/100]\n",
      "|batch[5/100]|train_batch_loss  0.35187948|\n",
      "accuracy:  0.15600820\n",
      "Epoch is [6/100]\n",
      "|batch[6/100]|train_batch_loss  0.33109005|\n",
      "accuracy:  0.18979059\n",
      "Epoch is [7/100]\n",
      "|batch[7/100]|train_batch_loss  0.32586018|\n",
      "accuracy:  0.20936867\n",
      "Epoch is [8/100]\n",
      "|batch[8/100]|train_batch_loss  0.30348479|\n",
      "accuracy:  0.31365243\n",
      "Epoch is [9/100]\n",
      "|batch[9/100]|train_batch_loss  0.29564691|\n",
      "accuracy:  0.32555805\n",
      "Epoch is [10/100]\n",
      "|batch[10/100]|train_batch_loss  0.29810313|\n",
      "accuracy:  0.35915013\n",
      "Epoch is [11/100]\n",
      "|batch[11/100]|train_batch_loss  0.27561715|\n",
      "accuracy:  0.36285154\n",
      "Epoch is [12/100]\n",
      "|batch[12/100]|train_batch_loss  0.27934311|\n",
      "accuracy:  0.43419880\n",
      "Epoch is [13/100]\n",
      "|batch[13/100]|train_batch_loss  0.25848208|\n",
      "accuracy:  0.42061514\n",
      "Epoch is [14/100]\n",
      "|batch[14/100]|train_batch_loss  0.24953723|\n",
      "accuracy:  0.44959843\n",
      "Epoch is [15/100]\n",
      "|batch[15/100]|train_batch_loss  0.24455905|\n",
      "accuracy:  0.49008437\n",
      "Epoch is [16/100]\n",
      "|batch[16/100]|train_batch_loss  0.21944990|\n",
      "accuracy:  0.55807885\n",
      "Epoch is [17/100]\n",
      "|batch[17/100]|train_batch_loss  0.23716401|\n",
      "accuracy:  0.55950302\n",
      "Epoch is [18/100]\n",
      "|batch[18/100]|train_batch_loss  0.21901426|\n",
      "accuracy:  0.57080862\n",
      "Epoch is [19/100]\n",
      "|batch[19/100]|train_batch_loss  0.21396925|\n",
      "accuracy:  0.60667593\n",
      "Epoch is [20/100]\n",
      "|batch[20/100]|train_batch_loss  0.19918287|\n",
      "accuracy:  0.62185269\n",
      "Epoch is [21/100]\n",
      "|batch[21/100]|train_batch_loss  0.19834116|\n",
      "accuracy:  0.63176790\n",
      "Epoch is [22/100]\n",
      "|batch[22/100]|train_batch_loss  0.18769708|\n",
      "accuracy:  0.66434742\n",
      "Epoch is [23/100]\n",
      "|batch[23/100]|train_batch_loss  0.18084403|\n",
      "accuracy:  0.68440652\n",
      "Epoch is [24/100]\n",
      "|batch[24/100]|train_batch_loss  0.16317170|\n",
      "accuracy:  0.71352238\n",
      "Epoch is [25/100]\n",
      "|batch[25/100]|train_batch_loss  0.16058053|\n",
      "accuracy:  0.72976952\n",
      "Epoch is [26/100]\n",
      "|batch[26/100]|train_batch_loss  0.16174069|\n",
      "accuracy:  0.71356883\n",
      "Epoch is [27/100]\n",
      "|batch[27/100]|train_batch_loss  0.14396876|\n",
      "accuracy:  0.74557523\n",
      "Epoch is [28/100]\n",
      "|batch[28/100]|train_batch_loss  0.14215336|\n",
      "accuracy:  0.75857691\n",
      "Epoch is [29/100]\n",
      "|batch[29/100]|train_batch_loss  0.13560190|\n",
      "accuracy:  0.76940334\n",
      "Epoch is [30/100]\n",
      "|batch[30/100]|train_batch_loss  0.14092639|\n",
      "accuracy:  0.76180397\n",
      "Epoch is [31/100]\n",
      "|batch[31/100]|train_batch_loss  0.13512819|\n",
      "accuracy:  0.78151782\n",
      "Epoch is [32/100]\n",
      "|batch[32/100]|train_batch_loss  0.13035931|\n",
      "accuracy:  0.79364693\n",
      "Epoch is [33/100]\n",
      "|batch[33/100]|train_batch_loss  0.11909453|\n",
      "accuracy:  0.80418621\n",
      "Epoch is [34/100]\n",
      "|batch[34/100]|train_batch_loss  0.11035872|\n",
      "accuracy:  0.81348062\n",
      "Epoch is [35/100]\n",
      "|batch[35/100]|train_batch_loss  0.10687267|\n",
      "accuracy:  0.84389791\n",
      "Epoch is [36/100]\n",
      "|batch[36/100]|train_batch_loss  0.12365458|\n",
      "accuracy:  0.79920164\n",
      "Epoch is [37/100]\n",
      "|batch[37/100]|train_batch_loss  0.10437928|\n",
      "accuracy:  0.82735992\n",
      "Epoch is [38/100]\n",
      "|batch[38/100]|train_batch_loss  0.10409676|\n",
      "accuracy:  0.83165344\n",
      "Epoch is [39/100]\n",
      "|batch[39/100]|train_batch_loss  0.10556575|\n",
      "accuracy:  0.83266745\n",
      "Epoch is [40/100]\n",
      "|batch[40/100]|train_batch_loss  0.10114536|\n",
      "accuracy:  0.84421852\n",
      "Epoch is [41/100]\n",
      "|batch[41/100]|train_batch_loss  0.10476268|\n",
      "accuracy:  0.84590127\n",
      "Epoch is [42/100]\n",
      "|batch[42/100]|train_batch_loss  0.08753262|\n",
      "accuracy:  0.87081741\n",
      "Epoch is [43/100]\n",
      "|batch[43/100]|train_batch_loss  0.07935351|\n",
      "accuracy:  0.87790947\n",
      "Epoch is [44/100]\n",
      "|batch[44/100]|train_batch_loss  0.07138529|\n",
      "accuracy:  0.89452296\n",
      "Epoch is [45/100]\n",
      "|batch[45/100]|train_batch_loss  0.07052003|\n",
      "accuracy:  0.89839967\n",
      "Epoch is [46/100]\n",
      "|batch[46/100]|train_batch_loss  0.08102534|\n",
      "accuracy:  0.88319552\n",
      "Epoch is [47/100]\n",
      "|batch[47/100]|train_batch_loss  0.07525736|\n",
      "accuracy:  0.89703503\n",
      "Epoch is [48/100]\n",
      "|batch[48/100]|train_batch_loss  0.06314359|\n",
      "accuracy:  0.91117566\n",
      "Epoch is [49/100]\n",
      "|batch[49/100]|train_batch_loss  0.06026622|\n",
      "accuracy:  0.91334061\n",
      "Epoch is [50/100]\n",
      "|batch[50/100]|train_batch_loss  0.06274923|\n",
      "accuracy:  0.90774057\n",
      "Epoch is [51/100]\n",
      "|batch[51/100]|train_batch_loss  0.05677190|\n",
      "accuracy:  0.91713961\n",
      "Epoch is [52/100]\n",
      "|batch[52/100]|train_batch_loss  0.05820875|\n",
      "accuracy:  0.91244213\n",
      "Epoch is [53/100]\n",
      "|batch[53/100]|train_batch_loss  0.06311959|\n",
      "accuracy:  0.89917529\n",
      "Epoch is [54/100]\n",
      "|batch[54/100]|train_batch_loss  0.07575388|\n",
      "accuracy:  0.88523559\n",
      "Epoch is [55/100]\n",
      "|batch[55/100]|train_batch_loss  0.06216822|\n",
      "accuracy:  0.90149976\n",
      "Epoch is [56/100]\n",
      "|batch[56/100]|train_batch_loss  0.06205470|\n",
      "accuracy:  0.90465125\n",
      "Epoch is [57/100]\n",
      "|batch[57/100]|train_batch_loss  0.05998905|\n",
      "accuracy:  0.91893985\n",
      "Epoch is [58/100]\n",
      "|batch[58/100]|train_batch_loss  0.05283042|\n",
      "accuracy:  0.92125620\n",
      "Epoch is [59/100]\n",
      "|batch[59/100]|train_batch_loss  0.05171547|\n",
      "accuracy:  0.92444511\n",
      "Epoch is [60/100]\n",
      "|batch[60/100]|train_batch_loss  0.05619844|\n",
      "accuracy:  0.91616777\n",
      "Epoch is [61/100]\n",
      "|batch[61/100]|train_batch_loss  0.05110649|\n",
      "accuracy:  0.92742013\n",
      "Epoch is [62/100]\n",
      "|batch[62/100]|train_batch_loss  0.04864945|\n",
      "accuracy:  0.92395960\n",
      "Epoch is [63/100]\n",
      "|batch[63/100]|train_batch_loss  0.04413797|\n",
      "accuracy:  0.93813693\n",
      "Epoch is [64/100]\n",
      "|batch[64/100]|train_batch_loss  0.04116346|\n",
      "accuracy:  0.94331828\n",
      "Epoch is [65/100]\n",
      "|batch[65/100]|train_batch_loss  0.03899607|\n",
      "accuracy:  0.94531061\n",
      "Epoch is [66/100]\n",
      "|batch[66/100]|train_batch_loss  0.04011141|\n",
      "accuracy:  0.94468918\n",
      "Epoch is [67/100]\n",
      "|batch[67/100]|train_batch_loss  0.04901574|\n",
      "accuracy:  0.93346668\n",
      "Epoch is [68/100]\n",
      "|batch[68/100]|train_batch_loss  0.04452866|\n",
      "accuracy:  0.94192311\n",
      "Epoch is [69/100]\n",
      "|batch[69/100]|train_batch_loss  0.04239053|\n",
      "accuracy:  0.94265781\n",
      "Epoch is [70/100]\n",
      "|batch[70/100]|train_batch_loss  0.04048964|\n",
      "accuracy:  0.93682180\n",
      "Epoch is [71/100]\n",
      "|batch[71/100]|train_batch_loss  0.04189807|\n",
      "accuracy:  0.93437907\n",
      "Epoch is [72/100]\n",
      "|batch[72/100]|train_batch_loss  0.03891121|\n",
      "accuracy:  0.94160806\n",
      "Epoch is [73/100]\n",
      "|batch[73/100]|train_batch_loss  0.04057337|\n",
      "accuracy:  0.93946753\n",
      "Epoch is [74/100]\n",
      "|batch[74/100]|train_batch_loss  0.04090239|\n",
      "accuracy:  0.93767759\n",
      "Epoch is [75/100]\n",
      "|batch[75/100]|train_batch_loss  0.04562706|\n",
      "accuracy:  0.94204716\n",
      "Epoch is [76/100]\n",
      "|batch[76/100]|train_batch_loss  0.04135983|\n",
      "accuracy:  0.94251538\n",
      "Epoch is [77/100]\n",
      "|batch[77/100]|train_batch_loss  0.03673996|\n",
      "accuracy:  0.94862305\n",
      "Epoch is [78/100]\n",
      "|batch[78/100]|train_batch_loss  0.03406604|\n",
      "accuracy:  0.95242829\n",
      "Epoch is [79/100]\n",
      "|batch[79/100]|train_batch_loss  0.03376916|\n",
      "accuracy:  0.95010230\n",
      "Epoch is [80/100]\n",
      "|batch[80/100]|train_batch_loss  0.03152220|\n",
      "accuracy:  0.95767670\n",
      "Epoch is [81/100]\n",
      "|batch[81/100]|train_batch_loss  0.03181664|\n",
      "accuracy:  0.95597840\n",
      "Epoch is [82/100]\n",
      "|batch[82/100]|train_batch_loss  0.03016037|\n",
      "accuracy:  0.96035934\n",
      "Epoch is [83/100]\n",
      "|batch[83/100]|train_batch_loss  0.03091485|\n",
      "accuracy:  0.95909445\n",
      "Epoch is [84/100]\n",
      "|batch[84/100]|train_batch_loss  0.03260833|\n",
      "accuracy:  0.95454925\n",
      "Epoch is [85/100]\n",
      "|batch[85/100]|train_batch_loss  0.03511787|\n",
      "accuracy:  0.95009824\n",
      "Epoch is [86/100]\n",
      "|batch[86/100]|train_batch_loss  0.03395504|\n",
      "accuracy:  0.95253942\n",
      "Epoch is [87/100]\n",
      "|batch[87/100]|train_batch_loss  0.03118648|\n",
      "accuracy:  0.95564645\n",
      "Epoch is [88/100]\n",
      "|batch[88/100]|train_batch_loss  0.02887399|\n",
      "accuracy:  0.96013378\n",
      "Epoch is [89/100]\n",
      "|batch[89/100]|train_batch_loss  0.03090275|\n",
      "accuracy:  0.96059337\n",
      "Epoch is [90/100]\n",
      "|batch[90/100]|train_batch_loss  0.03071184|\n",
      "accuracy:  0.95587519\n",
      "Epoch is [91/100]\n",
      "|batch[91/100]|train_batch_loss  0.02934167|\n",
      "accuracy:  0.95970626\n",
      "Epoch is [92/100]\n",
      "|batch[92/100]|train_batch_loss  0.03017522|\n",
      "accuracy:  0.96136810\n",
      "Epoch is [93/100]\n",
      "|batch[93/100]|train_batch_loss  0.02786544|\n",
      "accuracy:  0.95844526\n",
      "Epoch is [94/100]\n",
      "|batch[94/100]|train_batch_loss  0.02700184|\n",
      "accuracy:  0.96295847\n",
      "Epoch is [95/100]\n",
      "|batch[95/100]|train_batch_loss  0.02557025|\n",
      "accuracy:  0.96402929\n",
      "Epoch is [96/100]\n",
      "|batch[96/100]|train_batch_loss  0.02355423|\n",
      "accuracy:  0.96821835\n",
      "Epoch is [97/100]\n",
      "|batch[97/100]|train_batch_loss  0.02563417|\n",
      "accuracy:  0.96169279\n",
      "Epoch is [98/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|batch[98/100]|train_batch_loss  0.03521063|\n",
      "accuracy:  0.93642980\n",
      "Epoch is [99/100]\n",
      "|batch[99/100]|train_batch_loss  0.03865520|\n",
      "accuracy:  0.94198415\n",
      "Epoch is [100/100]\n",
      "|batch[100/100]|train_batch_loss  0.03527704|\n",
      "accuracy:  0.94996493\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#      train(fcn, False, 'checkpoint.pth')\n",
    "    train(unet, False, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2:  Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the validation method\n",
    "\n",
    "\"\"\"\n",
    "parameters:\n",
    "pretrained: the path for checkpoint file\n",
    "\"\"\"\n",
    "\n",
    "val_set = TrainDataset(data_val_path)\n",
    "val_data_loader = DataLoader(dataset=val_set, num_workers=num_workers, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "def validate(model, pretrained=None):\n",
    "    net = model.eval()\n",
    "    net.load_state_dict(t.load('{}.pth'.format(pretrained),map_location=device))\n",
    "    print('{}.pth'.format(pretrained))\n",
    "    \n",
    "    # Initialize the loss and accuracy for the model\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    batch_size=2\n",
    "    count = 0\n",
    "    index = 0\n",
    "    \n",
    "    \n",
    "    for iteration, sample in enumerate(val_data_loader):\n",
    "        img, mask = sample\n",
    "        img = Variable(img.unsqueeze(1).to(device))\n",
    "        mask = Variable(mask.to(device))\n",
    "\n",
    "        img = img / 255\n",
    "        \n",
    "        pred_mask = net(img)\n",
    "        pred_mask = F.log_softmax(pred_mask, dim=1)\n",
    "        mask = mask.to(device=device, dtype=torch.int64)\n",
    "\n",
    "        loss = criterion(pred_mask, mask)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        mask_image = torch.argmax(pred_mask.squeeze(), dim=1)\n",
    "        \n",
    "#         for i in range(batch_size):\n",
    "#         image = cv2.imread(os.path.join(data_val_path,'image','cmr'+str(index+101+i)+'.png'), cv2.IMREAD_UNCHANGED)\n",
    "#         correct_mask = cv2.imread(os.path.join(data_val_path,'mask','cmr'+str(index+101+i)+'_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        accuracy += categorical_dice(mask.cpu().numpy(), mask_image.detach().cpu().numpy())\n",
    "        count = count + 1\n",
    "        \n",
    "            \n",
    "        index += batch_size\n",
    "        \n",
    "        mask_acc = mask.squeeze().unsqueeze(2).cpu().numpy()\n",
    "        mask_image_acc = mask_image.unsqueeze(2).cpu().numpy()\n",
    "\n",
    "#         if mask_acc.shape == (batch_size, 96, 1, 96) and mask_image_acc.shape == (batch_size, 96, 1, 96):\n",
    "        \n",
    "#         show_image_mask(correct_mask, mask_image[i,...], cmap='gray')\n",
    "#         if iteration==0:\n",
    "#             mask_image_1=mask_image\n",
    "    \n",
    "    print(\"Val_loss is \" + str(val_loss/len(val_data_loader)))\n",
    "    print('accuracy: {: .8f}'.format(accuracy/count))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp.pth\n",
      "Val_loss is 0.21893372535705566\n",
      "accuracy:  0.64059868\n",
      "exp.pth\n",
      "Val_loss is 0.21893372014164925\n",
      "accuracy:  0.65339777\n",
      "exp.pth\n",
      "Val_loss is 0.21893372461199762\n",
      "accuracy:  0.63611408\n",
      "exp.pth\n",
      "Val_loss is 0.21893372088670732\n",
      "accuracy:  0.66017492\n",
      "exp.pth\n",
      "Val_loss is 0.21893372312188147\n",
      "accuracy:  0.63468988\n",
      "exp.pth\n",
      "Val_loss is 0.21893372535705566\n",
      "accuracy:  0.65829325\n",
      "exp.pth\n",
      "Val_loss is 0.21893371939659118\n",
      "accuracy:  0.64690026\n",
      "exp.pth\n",
      "Val_loss is 0.21893372386693954\n",
      "accuracy:  0.64997262\n",
      "exp.pth\n",
      "Val_loss is 0.21893372088670732\n",
      "accuracy:  0.63823052\n",
      "exp.pth\n",
      "Val_loss is 0.21893372461199762\n",
      "accuracy:  0.65372492\n",
      "exp.pth\n",
      "Val_loss is 0.21893372237682343\n",
      "accuracy:  0.65100784\n",
      "exp.pth\n",
      "Val_loss is 0.21893372386693954\n",
      "accuracy:  0.63980929\n",
      "exp.pth\n",
      "Val_loss is 0.21893372386693954\n",
      "accuracy:  0.63781003\n",
      "exp.pth\n",
      "Val_loss is 0.21893372312188147\n",
      "accuracy:  0.65931576\n",
      "exp.pth\n",
      "Val_loss is 0.21893372461199762\n",
      "accuracy:  0.62614036\n",
      "exp.pth\n",
      "Val_loss is 0.21893371865153313\n",
      "accuracy:  0.64581139\n",
      "exp.pth\n",
      "Val_loss is 0.21893372163176536\n",
      "accuracy:  0.63557550\n",
      "exp.pth\n",
      "Val_loss is 0.21893372461199762\n",
      "accuracy:  0.66060498\n",
      "exp.pth\n",
      "Val_loss is 0.21893372386693954\n",
      "accuracy:  0.65764905\n",
      "exp.pth\n",
      "Val_loss is 0.21893372163176536\n",
      "accuracy:  0.66296976\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    validate(unet, 'exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this block you are expected to write code to load saved model and deploy it to all data in test set to \n",
    "# produce segmentation masks in png images valued 0,1,2,3, which will be used for the submission to Kaggle.\n",
    "data_path = './data/test'\n",
    "num_workers = 0\n",
    "batch_size = 2\n",
    "net = unet.eval() \n",
    "net.load_state_dict(t.load('exp.pth',map_location=device)) # change the pretrained model path here\n",
    "test_set = TestDataset(data_path)\n",
    "test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
    "conv_initial = nn.Conv2d(1, 3, 1)\n",
    "data_save_path = './data/test/mask/'\n",
    "index = 0\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for iteration, sample in enumerate(test_data_loader):\n",
    "    img_test = sample\n",
    "\n",
    "    img_data = Variable(img_test.unsqueeze(1).to(device))\n",
    "    out = net(img_data)\n",
    "    out = F.log_softmax(out, dim=1)\n",
    "\n",
    "    mask_image = torch.argmax(out.squeeze(), dim=1)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "#         image = cv2.imread(os.path.join(data_path,'image','cmr'+str(index+121+i)+'.png'), cv2.IMREAD_UNCHANGED)\n",
    "#         show_image_mask(image, mask_image[i], cmap='gray')\n",
    "        cv2.imwrite(os.path.join(data_save_path, 'cmr'+str(index+121+i)+'_mask.png'), mask_image[i].detach().cpu().numpy())\n",
    "    \n",
    "    index += batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4, Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    *** Credit to https://www.kaggle.com/rakhlin/fast-run-length-encoding-python ***\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def submission_converter(mask_directory, path_to_save):\n",
    "    writer = open(os.path.join(path_to_save, \"submission.csv\"), 'w')\n",
    "    writer.write('id,encoding\\n')\n",
    "\n",
    "    files = os.listdir(mask_directory)\n",
    "\n",
    "    for file in files:\n",
    "        name = file[:-4]\n",
    "        mask = cv2.imread(os.path.join(mask_directory, file), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        mask1 = (mask == 1)\n",
    "        mask2 = (mask == 2)\n",
    "        mask3 = (mask == 3)\n",
    "\n",
    "        encoded_mask1 = rle_encoding(mask1)\n",
    "        encoded_mask1 = ' '.join(str(e) for e in encoded_mask1)\n",
    "        encoded_mask2 = rle_encoding(mask2)\n",
    "        encoded_mask2 = ' '.join(str(e) for e in encoded_mask2)\n",
    "        encoded_mask3 = rle_encoding(mask3)\n",
    "        encoded_mask3 = ' '.join(str(e) for e in encoded_mask3)\n",
    "\n",
    "        writer.write(name + '1,' + encoded_mask1 + \"\\n\")\n",
    "        writer.write(name + '2,' + encoded_mask2 + \"\\n\")\n",
    "        writer.write(name + '3,' + encoded_mask3 + \"\\n\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_converter(data_save_path, './data/test/submission_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
